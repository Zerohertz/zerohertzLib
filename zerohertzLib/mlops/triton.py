"""
MIT License

Copyright (c) 2023 Hyogeun Oh

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""

from abc import ABC, abstractmethod
from typing import Any

import tritonclient.grpc as grpcclient
from loguru import logger
from numpy.typing import DTypeLike, NDArray
from prettytable import PrettyTable
from tritonclient.utils import triton_to_np_dtype

try:
    import json
    import traceback

    import triton_python_backend_utils as pb_utils
except ImportError:
    pass


class TritonClientURL(grpcclient.InferenceServerClient):
    """ì™¸ë¶€ì—ì„œ ì‹¤í–‰ë˜ëŠ” triton inference serverì˜ í˜¸ì¶œì„ ìœ„í•œ class

    Args:
        url (``str``): í˜¸ì¶œí•  triton inference serverì˜ URL
        port (``int``): triton inference serverì˜ gRPC í†µì‹  port ë²ˆí˜¸
        verbose (``bool``): Verbose ì¶œë ¥ ì—¬ë¶€

    Methods:
        __call__:
            Model í˜¸ì¶œ ìˆ˜í–‰

            Args:
                model (``int | str``): í˜¸ì¶œí•  modelì˜ ì´ë¦„ ë° ID
                *args (``NDArray[DTypeLike]``): Model í˜¸ì¶œ ì‹œ ì‚¬ìš©ë  ì…ë ¥
                renew: (``bool | None``): ê° ëª¨ë¸ì˜ ìƒíƒœ ì¡°íšŒ ì‹œ ê°±ì‹  ì—¬ë¶€

            Returns:
                ``dict[str, NDArray[DTypeLike]]``: í˜¸ì¶œëœ modelì˜ ê²°ê³¼

    Examples:
        >>> tc = zz.mlops.TritonClientURL("localhost")
        >>> tc("YOLO", np.zeros((1, 3, 640, 640)))
        {'output0': array([[[3.90108061e+00, 3.51982164e+00, 7.49971962e+00, ...,
        2.21481919e-03, 1.17585063e-03, 1.36753917e-03]]], dtype=float32)}
    """

    def __init__(self, url: str, port: int = 8001, verbose: bool = False) -> None:
        self.url = f"{url}:{port}"
        super().__init__(url=self.url, verbose=verbose)
        self.configs = {}
        self.models = []
        for model in self.get_model_repository_index(as_json=True)["models"]:
            self.models.append(model["name"])
        self.emoji = {
            "LOADING": "ğŸš€",
            "READY": "âœ…",
            "UNLOADING": "ğŸ›Œ",
            "UNAVAILABLE": "ğŸ’¤",
        }

    def __call__(
        self,
        model: int | str,
        *args: NDArray[DTypeLike],
        renew: bool = False,
    ) -> dict[str, NDArray[DTypeLike]]:
        if isinstance(model, int):
            model = self.models[model]
        self._update_configs(model, renew)
        inputs = self.configs[model]["config"]["input"]
        outputs = self.configs[model]["config"]["output"]
        assert len(inputs) == len(args)
        triton_inputs = []
        for input_info, arg in zip(inputs, args):
            triton_inputs.append(self._set_input(input_info, arg))
        triton_outputs = []
        for output in outputs:
            triton_outputs.append(grpcclient.InferRequestedOutput(output["name"]))
        response = self.infer(
            model_name=model, inputs=triton_inputs, outputs=triton_outputs
        )
        response.get_response()
        triton_results = {}
        for output in outputs:
            triton_results[output["name"]] = response.as_numpy(output["name"])
        return triton_results

    def _update_configs(self, model: str, renew: bool) -> None:
        if renew or model not in self.configs:
            self.configs[model] = self.get_model_config(model, as_json=True)

    def _set_input(
        self, input_info: dict[str, list[int]], value: NDArray[DTypeLike]
    ) -> grpcclient._infer_input.InferInput:
        if "dims" in input_info.keys() and len(input_info["dims"]) != len(value.shape):
            logger.warning(
                "Expected dimension length of input (%d) does not match the input dimension length (%d) [input dimension: %s]",
                len(input_info["dims"]),
                len(value.shape),
                value.shape,
            )
        value = value.astype(triton_to_np_dtype(input_info["data_type"][5:]))
        return grpcclient.InferInput(
            input_info["name"],
            value.shape,
            input_info["data_type"][5:],
        ).set_data_from_numpy(value)

    def status(
        self,
        renew: bool = False,
        sortby: str = "STATE",
        reverse: bool = False,
    ) -> None:
        """Triton Inferece Serverì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜

        Args:
            renew: (``bool``): ê° ëª¨ë¸ì˜ ìƒíƒœ ì¡°íšŒ ì‹œ ê°±ì‹  ì—¬ë¶€
            sortby (``str``): ì •ë ¬ ê¸°ì¤€
            reverse (``bool``): ì •ë ¬ ì—­ìˆœ ì—¬ë¶€

        Examples:
            >>> tc.status()

            .. image:: _static/examples/static/mlops.TritonClientURL.status.gif
                :align: center
                :width: 700px
        """
        table = PrettyTable(
            ["STATE", "ID", "MODEL", "VERSION", "BACKEND", "INPUT", "OUTPUT"],
            title=f"Triton Inference Server Status [{self.url}]",
        )
        for model in self.get_model_repository_index(as_json=True)["models"]:
            if model["name"] not in self.models:
                self.models.append(model["name"])
            state = model.get("state", "UNAVAILABLE")
            if state in ["LOADING", "UNAVAILABLE"]:
                _input, _output = ["-"], ["-"]
                backend = "-"
            else:
                self._update_configs(model["name"], renew)
                _input, _output = [], []
                for inputs in self.configs[model["name"]]["config"]["input"]:
                    _input.append(
                        f"""{inputs["name"]} [{inputs["data_type"][5:]}: ({", ".join(inputs["dims"])})]"""
                    )
                for outputs in self.configs[model["name"]]["config"]["output"]:
                    _output.append(
                        f"""{outputs["name"]} [{outputs["data_type"][5:]}: ({", ".join(outputs["dims"])})]"""
                    )
                backend = self.configs[model["name"]]["config"].get("backend", "-")
            table.add_row(
                [
                    self.emoji[state],
                    self.models.index(model["name"]),
                    model["name"],
                    model.get("version", "-"),
                    backend,
                    "\n".join(_input),
                    "\n".join(_output),
                ]
            )
        if sortby:
            table.sortby = sortby
        table.reversesort = reverse
        logger.info("\n%s", str(table))

    def load_model(
        self,
        model_name: int | str,
        headers: str | None = None,
        config: str | None = None,
        files: str | None = None,
        client_timeout: float | None = None,
    ) -> None:
        """Triton Inference Server ë‚´ modelì„ loadí•˜ëŠ” í•¨ìˆ˜

        Args:
            model_name (``int | str``): Loadí•  modelì˜ ì´ë¦„ ë˜ëŠ” ID
            headers (``str | None``): Request ì „ì†¡ ì‹œ í¬í•¨í•  ì¶”ê°€ HTTP header
            config (``str | None``): Model load ì‹œ ì‚¬ìš©ë  config
            files (``str | None``): Model load ì‹œ override model directoryì—ì„œ ì‚¬ìš©í•  file
            client_timeout (``float | None``): ì´ˆ ë‹¨ìœ„ì˜ timeout

        Examples:
            >>> tc.load_model(0)
            >>> tc.load_model("MODEL_NAME")
        """
        if isinstance(model_name, int):
            model_name = self.models[model_name]
        super().load_model(model_name, headers, config, files, client_timeout)

    def unload_model(
        self,
        model_name: int | str,
        headers: str | None = None,
        unload_dependents: bool = False,
        client_timeout: float | None = None,
    ) -> None:
        """Triton Inference Server ë‚´ modelì„ unloadí•˜ëŠ” í•¨ìˆ˜

        Args:
            model_name (``int | str``): Unloadí•  modelì˜ ì´ë¦„ ë˜ëŠ” ID
            headers (``str | None``): Request ì „ì†¡ ì‹œ í¬í•¨í•  ì¶”ê°€ HTTP header
            unload_dependents (``bool``): Model unload ì‹œ dependentsì˜ unload ì—¬ë¶€
            client_timeout (``float | None``): ì´ˆ ë‹¨ìœ„ì˜ timeout

        Examples:
            >>> tc.unload_model(0)
            >>> tc.unload_model("MODEL_NAME")
        """
        if isinstance(model_name, int):
            model_name = self.models[model_name]
        super().unload_model(model_name, headers, unload_dependents, client_timeout)


class TritonClientK8s(TritonClientURL):
    """Kubernetesì—ì„œ ì‹¤í–‰ë˜ëŠ” triton inference serverì˜ í˜¸ì¶œì„ ìœ„í•œ class

    Args:
        svc_name (``str``): í˜¸ì¶œí•  triton inference serverì˜ Kubernetes serviceì˜ ì´ë¦„
        namespace (``str``): í˜¸ì¶œí•  triton inference serverì˜ namespace
        port (``int``): triton inference serverì˜ gRPC í†µì‹  port ë²ˆí˜¸
        verbose (``bool``): Verbose ì¶œë ¥ ì—¬ë¶€

    Methods:
        __call__:
            Model í˜¸ì¶œ ìˆ˜í–‰

            Args:
                model (``int | str``): í˜¸ì¶œí•  modelì˜ ì´ë¦„ ë˜ëŠ” ID
                *args (``NDArray[DTypeLike]``): Model í˜¸ì¶œ ì‹œ ì‚¬ìš©ë  ì…ë ¥
                renew: (``bool | None``): ê° ëª¨ë¸ì˜ ìƒíƒœ ì¡°íšŒ ì‹œ ê°±ì‹  ì—¬ë¶€

            Returns:
                ``dict[str, NDArray[DTypeLike]]``: í˜¸ì¶œëœ modelì˜ ê²°ê³¼

    Examples:
        Kubernetes:
            >>> kubectl get svc -n yolo
            NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
            fastapi-svc                   ClusterIP   10.106.72.126   <none>        80/TCP     90s
            triton-inference-server-svc   ClusterIP   10.96.28.172    <none>        8001/TCP   90s
            >>> docker exec -it ${API_CONTAINER} bash

        Python:
            >>> tc = zz.mlops.TritonClientK8s("triton-inference-server-svc", "yolo")
            >>> tc("YOLO", np.zeros((1, 3, 640, 640)))
            {'output0': array([[[3.90108061e+00, 3.51982164e+00, 7.49971962e+00, ...,
            2.21481919e-03, 1.17585063e-03, 1.36753917e-03]]], dtype=float32)}
    """

    def __init__(
        self,
        svc_name: str,
        namespace: str,
        port: int = 8001,
        verbose: bool = False,
    ) -> None:
        super().__init__(f"{svc_name}.{namespace}", port, verbose)


class BaseTritonPythonModel(ABC):
    """Triton Inference Serverì—ì„œ Python backend ì‚¬ìš©ì„ ìœ„í•œ class

    Note:
        Abstract Base Class: Modelì˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” abstract method ``_inference`` ì •ì˜ í›„ ì‚¬ìš©

    Hint:
        Loggerì˜ ìƒ‰ìƒ ì ìš©ì„ ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ í™˜ê²½ ë³€ìˆ˜ ì •ì˜ í•„ìš”

        .. code-block:: yaml

            spec:
              template:
                spec:
                  containers:
                    - name: ${NAME}
                      ...
                      env:
                        - name: "FORCE_COLOR"
                          value: "1"
                      ...

    Methods:
        _inference:
            Model ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” private method (ìƒì†ì„ í†µí•œ ì¬ì •ì˜ í•„ìˆ˜)

            Args:
                inputs (``NDArray[DTypeLike]``): Model ì¶”ë¡  ì‹œ ì‚¬ìš©ë  ì…ë ¥ (``config.pbtxt`` ì˜ ì…ë ¥ì— ë”°ë¼ ì…ë ¥ ê²°ì •)

            Returns:
                ``NDArray[DTypeLike] | tuple[NDArray[DTypeLike]]``: Modelì˜ ì¶”ë¡  ê²°ê³¼

    Examples:
        ``model.py``:
            .. code-block:: python

                class TritonPythonModel(zz.mlops.BaseTritonPythonModel):
                    def initialize(self, args):
                        super().initialize(args, 10)
                        self.model = Model(cfg)

                    def _inference(self, input_image):
                        return self.model(input_image)

        Normal Logs:
            .. code-block:: apl

                [04/04/24 00:00:00] INFO     [MODEL] Initialize                        triton.py:*
                [04/04/24 00:00:00] INFO     [MODEL] Called                            triton.py:*
                                    DEBUG    [MODEL] inputs: (3, 3, 3)                 triton.py:*
                                    INFO     [MODEL] Inference start                   triton.py:*
                                    DEBUG    [MODEL] outputs: (10,) (20,)              triton.py:*
                                    INFO     [MODEL] Inference completed               triton.py:*

        Error Logs:
            .. code-block:: apl

                [04/04/24 00:00:00] INFO     [MODEL] Called                            triton.py:*
                                    INFO     [MODEL] Inference start                   triton.py:*
                                    CRITICAL [MODEL] Hello, World!                     triton.py:*
                                            ====================================================================================================
                                            Traceback (most recent call last):
                                            File "/usr/local/lib/python3.8/dist-packages/zerohertzLib/mlops/triton.py", line *, in execute
                                                outputs = self._inference(*inputs)
                                            File "/models/model/*/model.py", line *, in _inference
                                                raise Exception("Hello, World!")
                                            Exception: Hello, World!
                                            ====================================================================================================
    """

    def initialize(self, args: dict[str, Any]) -> None:
        """Triton Inference Server ì‹œì‘ ì‹œ ìˆ˜í–‰ë˜ëŠ” method

        Args:
            args (``dict[str, Any]``): ``config.pbtxt`` ì— í¬í•¨ëœ modelì˜ ì •ë³´
        """
        self.cfg = json.loads(args["model_config"])
        logger.info("Initialize")

    def execute(self, requests: list[Any]) -> list[Any]:
        """Triton Inference Server í˜¸ì¶œ ì‹œ ìˆ˜í–‰ë˜ëŠ” method

        Args:
            requests (``list[Any]``): Clientì—ì„œ ì „ì†¡ëœ model inputs

        Returns:
            ``list[pb_utils.InferenceResponse]``: Clientì— ì‘ë‹µí•  modelì˜ ì¶”ë¡  ê²°ê³¼
        """
        responses = []
        for request in requests:
            try:
                logger.info("Called")
                inputs = self._get_inputs(request)
                logger.debug(
                    "inputs: %s", " ".join([str(input_.shape) for input_ in inputs])
                )
                logger.info("Inference start")
                outputs = self._inference(*inputs)
                if not isinstance(outputs, tuple):
                    outputs = tuple([outputs])
                logger.debug(
                    "outputs: %s", " ".join([str(output.shape) for output in outputs])
                )
                logger.info("Inference completed")
                response = self._set_outputs(outputs)
                responses.append(response)
            except Exception as error:
                message = (
                    str(error)
                    + "\n"
                    + "=" * 100
                    + "\n"
                    + str(traceback.format_exc())
                    + "=" * 100
                )
                logger.critical(message)
                responses.append(
                    pb_utils.InferenceResponse(
                        output_tensors=[], error=pb_utils.TritonError(message)
                    )
                )
        return responses

    def _get_inputs(self, request: Any) -> list[NDArray[DTypeLike]]:
        inputs = []
        for input_ in self.cfg["input"]:
            inputs.append(
                pb_utils.get_input_tensor_by_name(request, input_["name"]).as_numpy()
            )
        return inputs

    def _set_outputs(self, outputs: tuple[NDArray[DTypeLike]]) -> Any:
        output_tensors = []
        for output, value in zip(self.cfg["output"], outputs):
            output_tensors.append(
                pb_utils.Tensor(
                    output["name"],
                    value.astype(pb_utils.triton_string_to_numpy(output["data_type"])),
                )
            )
        return pb_utils.InferenceResponse(output_tensors=output_tensors)

    @abstractmethod
    def _inference(
        self, *inputs: NDArray[DTypeLike]
    ) -> NDArray[DTypeLike] | tuple[NDArray[DTypeLike]]:
        return inputs

    def finalize(self) -> None:
        """Triton Inference Server ì¢…ë£Œ ì‹œ ìˆ˜í–‰ë˜ëŠ” method"""
        logger.info("Finalize")
